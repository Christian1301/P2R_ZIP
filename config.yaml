# ========================================
# CONFIGURAZIONE FINALE P2R + ZIP
# ========================================

RUN_NAME: "shha_test" 
SEED: 2025
DEVICE: "cuda"

# === DATASET ===
DATASET: "shha"
DATA:
  ROOT: "/mnt/localstorage/cromano/Datasets/ShanghaiTech/part_A"
  ZIP_BLOCK_SIZE: 16
  IMG_EXTS: [".jpg", ".png"]
  NORM_MEAN: [0.485, 0.456, 0.406]
  NORM_STD:  [0.229, 0.224, 0.225]
  TRAIN_SPLIT: "train_data"
  VAL_SPLIT: "val_data"
  CROP_SIZE: 256 # default per Stage 2 / Stage 3 P2R
  CROP_SCALE: [0.75, 2.0]
  CROP_SIZE_STAGE1: 448
  CROP_SCALE_STAGE1: [0.75, 2.0]
  CROP_SIZE_STAGE2: 256
  CROP_SCALE_STAGE2: [0.75, 2.0]

# === MODELLO ===
MODEL:
  BACKBONE: "vgg16_bn"
  ZIP_PI_THRESH: 0.15
  ZIP_PI_THRESH_START: 0.02
  ZIP_PI_THRESH_WARMUP_EPOCHS: 200
  ZIP_PI_MODE: "soft"
  ZIP_PI_MODE_STAGE2: "prob"  # Stage 2 richiede gating morbido per evitare maschere quasi nulle
  ZIP_PI_THRESH_STAGE2: 0.02   # Soglia bloccata a un valore permissivo durante il training P2R
  ZIP_PI_THRESH_STAGE2_START: 0.02
  ZIP_PI_THRESH_STAGE2_WARMUP_EPOCHS: 0
  ZIP_PI_MODE_STAGE3: "prob"   # Mantieni gating permissivo anche nel joint fine-tuning
  ZIP_PI_THRESH_STAGE3: 0.02
  ZIP_PI_FLOOR_STAGE2: 0.05
  ZIP_LAMBDA_CLAMP_STAGE2: [0.0, 2.5]
  ZIP_PI_FLOOR_STAGE3: 0.05
  ZIP_LAMBDA_CLAMP_STAGE3: [0.0, 2.5]

  GATE: "multiply"
  UPSAMPLE_TO_INPUT: true

ZIP_HEAD:
  LAMBDA_SCALE: 0.5
  LAMBDA_MAX: 3.0
  USE_SOFTPLUS: true
  LAMBDA_NOISE_STD: 0.0

# === STAGE 1: ZIP TRAINING  ===
OPTIM_ZIP:
  BATCH_SIZE: 8 # MODIFICATO (da 4) - Allineato a sha.yaml
  NUM_WORKERS: 4
  EPOCHS: 1300 
  WEIGHT_DECAY: 1.0e-4
  SCHEDULER: "cosine"
  WARMUP_EPOCHS: 10
  LR_MIN_FACTOR: 0.05
  VAL_INTERVAL: 2
  RESUME_LAST: false
  SAVE_BEST: true
  EARLY_STOPPING_PATIENCE: 200
  CLIP_GRAD_NORM: 1.0
  BASE_LR: 1.0e-4 # alzato per recuperare la capacità di apprendere pattern di densità
  LR_BACKBONE: 1.0e-5 # mantieni backbone più lento ma non completamente congelato

ZIP_LOSS:
  WEIGHT_CE: 3.0
  WEIGHT_NLL: 1.0
  WEIGHT_COUNT: 0.25

ZIP_REG:
  PI_TARGET: 0.28
  PI_WEIGHT: 5.0e-3
  LAMBDA_TARGET: 1.0
  LAMBDA_WEIGHT: 2.2e-3

# === STAGE 2: P2R TRAINING ===
OPTIM_P2R:
  BATCH_SIZE: 16
  NUM_WORKERS: 4
  EPOCHS: 1500
  LR: 5.0e-5 # MODIFICATO (da 1.0e-4) - Allineato a config.py
  FINETUNE_BACKBONE: false
  LR_BACKBONE: 1.0e-5 # MODIFICATO (da 5.0e-6) - Allineato a config.py
  WEIGHT_DECAY: 1.0e-4
  SCHEDULER: "cosine"
  VAL_INTERVAL: 10
  RESUME_LAST: true
  SAVE_BEST: true

P2R_LOSS:
  MU: 128
  TAU: 8
  SCALE_WEIGHT: 0.15
  POS_WEIGHT: 3.2
  CHUNK_SIZE: 2048
  MIN_RADIUS: 6.0
  MAX_RADIUS: 72.0
  LOG_SCALE_INIT: 0.0
  LOG_SCALE_LR_FACTOR: 0.05
  LOG_SCALE_CLAMP: [-0.5, 3.0]
  LOG_SCALE_CALIBRATION_MAX_DELTA: 4.0
  CALIBRATE_BATCHES: 12

# === STAGE 3: JOINT TRAINING ===
OPTIM_JOINT:
  BATCH_SIZE: 8 
  NUM_WORKERS: 4
  EPOCHS: 800
  LR_HEADS: 1.5e-4
  LR_BACKBONE: 1.5e-5
  WEIGHT_DECAY: 1.0e-4
  SCHEDULER: "cosine"
  SCHEDULER_STEP: "epoch"
  WARMUP_EPOCHS: 5
  LR_MIN_FACTOR: 0.02
  FINE_TUNE_LR_SCALE: 1.0
  LOAD_STAGE3_BEST: true
  EARLY_STOPPING_PATIENCE: 50
  VAL_INTERVAL: 5
  RESUME_LAST: false
  SAVE_BEST: true

JOINT_LOSS:
  ALPHA: 3.2
  COUNT_L1_W: 0.05
  ZIP_SCALE: 0.25

# === BIN CONFIGURATION ===
BINS_CONFIG:
  shha:
    bins: [[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6], [7, 7], [8, 8], [9, 9], [10, 10], [11, 12], [13, 14], [15, 9999]]
    bin_centers: [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.38, 13.38, 16.26]
  
EXP:
  OUT_DIR: "exp"
  SAVE_BEST: true